---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown] Notebook examining media house value (medv) per the example in Chapter 3.6.2 of "An Introduction to Statistical Learning" (James, Witten, Hastie & Tibshirani).

First, we read in the required libraries: MASS and ISLR.  ISLR contains the datasets used in the aforementioned textbook. 

```{r}
library(MASS)
library(ISLR)
```

We are going to try to predict median home value using 13 predictors in the "Boston" dataset from the MASS library.  We use the fix command to fix the dataset, and then use the "names" function to retrieve the feature names.

```{r}
fix(Boston)
names(Boston)
```
Check the distribution of median home values first:

```{r}
hist(Boston$medv)
```


We start by performing a linear (least squares) regression using the lm() function. We first create a model using lstat (the % households with a low socioeconomic status) as the sole predictor.

```{r}
lm.fit=lm(medv~lstat, data=Boston)
#print out some of the basic info about the model lm.fit
lm.fit
```

To get more information about this model, we do the following:

```{r}
summary(lm.fit)
```

We can find the details about what pieces of information are in the lm.fit object by calling the names() function again:

```{r}
names(lm.fit)
```

We can access any of these features using the notation lmfit$feature.  For example:

```{r}
lm.fit$coefficients
```

Observe the confidence intervals on the model's intercept and slope using the confint() function:

```{r}
confint(lm.fit)
```

Furthermore, we can use the lm.fit model to make a prediction based on specified lstat values.  For example,

```{r}
lstat_test = c(5,10,15)
predict(lm.fit, data.frame(lstat_test), interval="confidence")
```

We can also plot the data:
```{r}
plot(Boston$lstat,Boston$medv) #scatterplot of the data
abline(lm.fit) #draws the least squares line
```

```{r}
#add some color to the plot
percentage_low_socioeconomic_households = Boston$lstat
median_home_value = Boston$medv
plot(percentage_low_socioeconomic_households,median_home_value, col="red", pch=20) #scatterplot of the data
abline(lm.fit, lwd=3) #draws the least squares line
```

Next, we take a look at some diagnostic plots associated with the model lm.fit:

```{r}
par(mfrow=c(2,2))
plot(lm.fit)
```
We observe some non-linearity in this model based on the residuals plot (it is curved, not flat and evenly distributed). Next, we will try to improve the linear model by incorporating additional features. That will be in the next session.

```{r}
plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit))
```

Let's add in the age independent variable to make a model with two predictors.
```{r}
lm.fit = lm(medv~lstat+age, data=Boston)
summary(lm.fit)
```

Now we can try fitting a model with all 13 variables:

```{r}
lm.fit=lm(medv~., data=Boston)
summary(lm.fit)
```
Check the R-squared value in the model.
```{r}
summary(lm.fit)$r.sq
```
Check the residual standard error (RSE) (note that this measure is in the units of y (median home value in thousands of dollars):
```{r}
summary(lm.fit)$sigma
```


We can check the variance inflation factor (VIF) for each feature to check for multicolinearity.  This function is available through the car library.

```{r}
library(car)
vif(lm.fit)
```
Test out a new model using lstat, age, and lstat x age as predictors:
```{r}
lm.fit1 = lm(medv~lstat*age, data=Boston)
summary(lm.fit1)
```

We can also try to use non-linear transformations of the predictors in our model.  For example, we can create a quadratic predictor.
```{r}
lm.fit2 = lm(medv~lstat+I(lstat^2), data=Boston)
summary(lm.fit2)
```
```{r}
par(mfrow=c(2,2))
plot(lm.fit2)
```
The residuals seem flat and without a trend/pattern.  This indicates that the quadratic term addition resulted in a better fit. 
